
task:
  yaw_lambda: -10.0 
  reward_scales:
    target: 20.0
    # go_forward: 1.0 
    stable: 0.01 
    yaw: 0.001 
    angular: 0.03
    crash: 10 
    # altitude_hold: 1
    lazy: 1
    safe: 0.0

  termination_if_roll_greater_than: 180   # degree
  termination_if_pitch_greater_than: 180 
  termination_if_close_to_ground: 0.2 
  termination_if_x_greater_than: 3.5 
  termination_if_y_greater_than: 3.5 
  termination_if_z_greater_than: 1.0
  collision_if_dis_less_than: 0.05

  clip_actions: 1.0 
  target_thr: 0.1
  episode_length_s: 15.0 
  max_episode_length: 1000

  num_actions: 4                      # roll, pitch, yaw, thrust
  num_commands: 3                     # 3d position: x, y, z
  command_cfg:
    pos_x_range: [-1.0, -0.2] 
    pos_y_range: [-1.2, 1.2] 
    pos_z_range: [0.7, 1.1] 

  num_obs_state: 17                   # quat*4, ang_rate*3, lin_acc*3, last_action*4 
  num_obs_img_raw: [1, 12, 16]        # raw is[1, 48, 64], after pooling is [1, 12, 16]
  num_obs_img_pooling: [1, 12, 16]    # raw is[1, 48, 64], after pooling is [1, 12, 16]
  num_obs_privileged: 9               # pos*3, target_pos*3, lin_vel*3
  num_hiden_state: 192

  obs_scales:
    cur_pos_error: 0.3333333333333333   # 1 / 3.0 
    lin_vel: 0.3333333333333333   # 1 / 3.0 
    ang_vel: 0.3183098861837907   # 1 / 3.14159 


# All the settings for training the model are defined here
train:
  checkpoint: -1 
  experiment_name: drone-hovering
  load_run: -1 
  log_interval: 1 
  num_steps_per_env: 80
  max_iterations: 4000
  record_interval: -1 
  resume: False 
  resume_path: null
  run_name:  
  runner_class_name: runner_class_name 
  save_interval: 5499 
  obs_groups: {"policy": ["state", "depth"], "critic": ["state", "depth"]} # maps observation groups to types. See `vec_env.py` for more information


  algorithm: 
    class_name: PPO 
    clip_param: 0.2 
    desired_kl: 0.01 
    entropy_coef: 0.004 
    gamma: 0.99 
    lam: 0.95 
    learning_rate: 0.0003 
    max_grad_norm: 1.0 
    num_learning_epochs: 5 
    num_mini_batches: 4 
    schedule: adaptive 
    use_clipped_value_loss: True 
    value_loss_coef: 1.0 
          
  init_member_classes: {}

  policy: 
    class_name: ActorCriticCustom 
    activation: tanh 
    actor_hidden_dims: [128, 128] 
    critic_hidden_dims: [256, 256]
    init_noise_std: 0.2
    # noise_std_type: "log"

  runner_class_name: OnPolicyRunner 
  seed: 1 
