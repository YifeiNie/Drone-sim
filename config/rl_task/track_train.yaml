# All the settings for training the model are defined here


algorithm: 
  clip_param: 0.2 
  desired_kl: 0.01 
  entropy_coef: 0.004 
  gamma: 0.99 
  lam: 0.95 
  learning_rate: 0.0004 
  max_grad_norm: 1.0 
  num_learning_epochs: 5 
  num_mini_batches: 4 
  schedule: adaptive 
  use_clipped_value_loss: True 
  value_loss_coef: 1.0 
        
init_member_classes: {}

policy: 
  activation: tanh 
  actor_hidden_dims: [128, 128] 
  critic_hidden_dims: [128, 128] 
  init_noise_std: 10.0 

runner: 
  algorithm_class_name: PPO 
  checkpoint: -1 
  experiment_name: drone-hovering
  load_run: -1 
  log_interval: 1 
  max_iterations: 500 
  num_steps_per_env: 200 
  policy_class_name: ActorCritic 
  record_interval: -1 
  resume: False 
  resume_path: null
  run_name:  
  runner_class_name: runner_class_name 
  save_interval: 100 

runner_class_name: OnPolicyRunner 
seed: 1 

